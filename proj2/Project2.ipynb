{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear).__init__()\n",
    "\n",
    "        self.name = 'Linear'\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight = FloatTensor(in_features, out_features)\n",
    "        self.weight_grad = FloatTensor(in_features, out_features)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = FloatTensor(out_features)\n",
    "            self.bias_grad = FloatTensor(out_features)\n",
    "        else:\n",
    "            self.bias = None\n",
    "            self.bias_grad = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.uniform_(-stdv, stdv)\n",
    "        self.weight_grad.fill_(0)\n",
    "        if self.bias is not None:\n",
    "            self.bias.uniform_(-stdv, stdv)\n",
    "            self.bias_grad.fill_(0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Y = X * W + b\n",
    "        self.input = input.clone()\n",
    "        if self.bias is not None:\n",
    "            return self.input.matmul(self.weight).add(self.bias)\n",
    "        else:\n",
    "            return self.input.matmul(self.weight)\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        # dW = X^T * dL/dY\n",
    "        self.weight_grad = self.input.t().matmul(gradwrtoutput)\n",
    "        # db = (dL/dY)^T * 1\n",
    "        if self.bias is not None:\n",
    "            self.bias_grad = gradwrtoutput.t().sum(1)\n",
    "        # dX = dL/dY * W^T\n",
    "        return gradwrtoutput.matmul(self.weight.t())\n",
    "\n",
    "    def param(self):\n",
    "        if self.bias is not None:\n",
    "            return [(self.weight, self.weight_grad),\n",
    "                    (self.bias, self.bias_grad)]\n",
    "        else:\n",
    "            return [(self.weight, self.weight_grad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU).__init__()\n",
    "\n",
    "        self.name = 'ReLU'\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Y = max(0,X)\n",
    "        self.input = input.clone()\n",
    "        return self.input.mul(self.input.gt(0).float())\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        if self.input is not None:\n",
    "            return gradwrtoutput.mul(self.input.gt(0).float())\n",
    "        else:\n",
    "            print(\"Forward First\")\n",
    "            return None\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh).__init__()\n",
    "        self.name = 'Tanh'\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Y = (exp(X) - exp(-X))/(exp(X) + exp(-X))\n",
    "        self.input = input.clone()\n",
    "        return self.input.tanh()\n",
    "        # self.input.masked_fill_(self.input.gt(50), 50)  # prevent overflow\n",
    "        # return (self.input.exp() - self.input.mul(-1).exp()) / (self.input.exp() + self.input.mul(-1).exp())\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        # dY/dX = 4/(exp(x) + exp(-x))^2\n",
    "        if self.input is not None:\n",
    "            grad = 4. / (self.input.exp() + self.input.mul(-1).exp()).pow(2)\n",
    "            return gradwrtoutput.mul(grad)\n",
    "        else:\n",
    "            print(\"Forward First\")\n",
    "            return None\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self):\n",
    "        super(Sequential).__init__()\n",
    "        self.name = 'Sequential'\n",
    "        self.module_list = []\n",
    "\n",
    "    def add(self, *module):\n",
    "        for m in module:\n",
    "            self.module_list.append(m)\n",
    "\n",
    "    def forward(self, input):\n",
    "        module_input = input.clone()\n",
    "        for module in self.module_list:\n",
    "            module_output = module.forward(module_input)\n",
    "            module_input = module_output\n",
    "        return module_output\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        grad = gradwrtoutput\n",
    "        for module in self.module_list[::-1]:\n",
    "            grad = module.backward(grad)\n",
    "        # return grad\n",
    "\n",
    "    def param(self):\n",
    "        param_list = []\n",
    "        for module in self.module_list:\n",
    "            param_list.append(module.param())\n",
    "        return param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE():\n",
    "    def __init__(self):\n",
    "        super(LossMSE).__init__()\n",
    "        self.name = 'LossMSE'\n",
    "\n",
    "    def calculate(self, predict_value, true_value):\n",
    "        self.predict_value = predict_value\n",
    "        self.true_value = true_value\n",
    "        self.loss = (self.predict_value - self.true_value).float().pow(2).mean()\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self):\n",
    "        return 2 * (self.predict_value - self.true_value).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(label):\n",
    "    n = label.size(0)\n",
    "    y = FloatTensor(n, 2)\n",
    "    y[:, 0] = 2*(0.5-label)\n",
    "    y[:, 1] = -y[:, 0]\n",
    "    return y.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = FloatTensor(1000, 2).uniform_(0, 1)\n",
    "y_train = onehot(X_train.pow(2).sum(1).lt(1./2./math.pi).float())\n",
    "\n",
    "X_test = FloatTensor(1000, 2).uniform_(0, 1)\n",
    "y_test = onehot(X_test.pow(2).sum(1).lt(1./2./math.pi).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "mu, std = X_train.mean(), X_train.std()\n",
    "X_train.sub_(mu).div_(std)\n",
    "X_test.sub_(mu).div_(std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Linear(2, 25), ReLU(), Linear(25, 2), Tanh())\n",
    "model.add(Linear(2, 25), ReLU(), Linear(25, 25), ReLU(), Linear(25, 2), Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Train Error: 87.50%\n"
     ]
    }
   ],
   "source": [
    "train_output = model.forward(X_train)\n",
    "print(\"Before Train Error: {:.2%}\".format(train_output.max(1)[1].ne(y_train.max(1)[1]).sum()/train_output.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3163531164079902\n",
      "0 0.5981303619593382\n",
      "0 0.17657488716766237\n",
      "0 0.2967188342334703\n",
      "0 0.5641494222125039\n",
      "0 0.4059076734725386\n",
      "0 0.5075615203566849\n",
      "0 0.3818721437267959\n",
      "0 0.3098921576794237\n",
      "0 0.40196859633550047\n",
      "0 0.3586356541933492\n",
      "0 0.5070931748510339\n",
      "0 0.2873914254514966\n",
      "0 0.2265728721719279\n",
      "0 0.14733040874720246\n",
      "0 0.1351410108260916\n",
      "0 0.06620232279114134\n",
      "0 0.28312293126427845\n",
      "0 0.614935468473384\n",
      "0 0.19763572845208854\n",
      "1 0.12784096945996168\n",
      "1 0.14406644933699694\n",
      "1 0.12151017749447877\n",
      "1 0.10112280967699683\n",
      "1 0.10392652383025147\n",
      "1 0.06023864215335593\n",
      "1 0.17622976106863\n",
      "1 0.42837168511010815\n",
      "1 0.15883353567652694\n",
      "1 0.3789408412552515\n",
      "1 0.31704214701678224\n",
      "1 0.2718159648463718\n",
      "1 0.13201223012930766\n",
      "1 0.08702135522290845\n",
      "1 0.06776381217141825\n",
      "1 0.07812804657289597\n",
      "1 0.03846169431679378\n",
      "1 0.10049411302993498\n",
      "1 0.08271483717271838\n",
      "1 0.03116583274173653\n",
      "2 0.04089468789543162\n",
      "2 0.02440970408862604\n",
      "2 0.09447796186971441\n",
      "2 0.18074463179181827\n",
      "2 0.10443046609291078\n",
      "2 0.15605375125700746\n",
      "2 0.09085728380690196\n",
      "2 0.2639787940527388\n",
      "2 0.022565323973271312\n",
      "2 0.07432547844218237\n",
      "2 0.13636731808942065\n",
      "2 0.2656101834536662\n",
      "2 0.23894891548798414\n",
      "2 0.17844757865281036\n",
      "2 0.06920727232635933\n",
      "2 0.03305755477070978\n",
      "2 0.009263617874635201\n",
      "2 0.11646343322210527\n",
      "2 0.010147805326452968\n",
      "2 0.046205705693589574\n",
      "3 0.09463966872919205\n",
      "3 0.023306164477275182\n",
      "3 0.04728136294284923\n",
      "3 0.10189548072182354\n",
      "3 0.026972827890173932\n",
      "3 0.009257366354118802\n",
      "3 0.050213362297148054\n",
      "3 0.2256593133710801\n",
      "3 0.05202190908023759\n",
      "3 0.05271255551243357\n",
      "3 0.054411484070597496\n",
      "3 0.19008836298226567\n",
      "3 0.14151200839113465\n",
      "3 0.010214889598547145\n",
      "3 0.003482174176541086\n",
      "3 0.0036000848625305794\n",
      "3 0.0036734491813584925\n",
      "3 0.0930907865212789\n",
      "3 0.010346906995583502\n",
      "3 0.03530171128974036\n",
      "4 0.07811712917879386\n",
      "4 0.02115135434728728\n",
      "4 0.02268733632043194\n",
      "4 0.04684662812824513\n",
      "4 0.0125688559567676\n",
      "4 0.008136410030806224\n",
      "4 0.03271648906657266\n",
      "4 0.12801991406796132\n",
      "4 0.14940660228091582\n",
      "4 0.11384698464616466\n",
      "4 0.03334460673096512\n",
      "4 0.0828881226799335\n",
      "4 0.09470523077997249\n",
      "4 0.17286583236859282\n",
      "4 0.0051302284208914984\n",
      "4 0.0033845042582196783\n",
      "4 0.003751939802977766\n",
      "4 0.09363072015584198\n",
      "4 0.002726779077274557\n",
      "4 0.04585379574464106\n",
      "5 0.0420539287416317\n",
      "5 0.023140123386661562\n",
      "5 0.03307802308697113\n",
      "5 0.05101848705366272\n",
      "5 0.007398997830656597\n",
      "5 0.0036083413693068778\n",
      "5 0.017494876566281833\n",
      "5 0.02695607175979962\n",
      "5 0.04911544373673237\n",
      "5 0.040427834823756045\n",
      "5 0.07131586778534534\n",
      "5 0.1215301703804445\n",
      "5 0.06896548130679558\n",
      "5 0.021935986174355902\n",
      "5 0.0033150824116930265\n",
      "5 0.00211530994440146\n",
      "5 0.007747629264749847\n",
      "5 0.09515367521932727\n",
      "5 0.0010366128538087693\n",
      "5 0.03629309758523288\n",
      "6 0.01828916295954638\n",
      "6 0.018989732608832455\n",
      "6 0.018456307124554882\n",
      "6 0.030437786616889433\n",
      "6 0.005934741682470026\n",
      "6 0.0016060281746333872\n",
      "6 0.013342640750608332\n",
      "6 0.011806957135520869\n",
      "6 0.032646867495648595\n",
      "6 0.04018952911777184\n",
      "6 0.19798554154072684\n",
      "6 0.01990051384492464\n",
      "6 0.044088987808010814\n",
      "6 0.05576079891814739\n",
      "6 0.003222993904084852\n",
      "6 0.002112627900161215\n",
      "6 0.0007170960107880475\n",
      "6 0.113585331590434\n",
      "6 0.0010067254708584273\n",
      "6 0.03422432931950862\n",
      "7 0.05186168996346982\n",
      "7 0.01303203158766003\n",
      "7 0.002418238761606979\n",
      "7 0.01042028090167733\n",
      "7 0.02562790149138305\n",
      "7 0.001048818137233063\n",
      "7 0.024449765922022876\n",
      "7 0.027073352498314593\n",
      "7 0.05690469010671702\n",
      "7 0.06247478979635254\n",
      "7 0.1977319079666953\n",
      "7 0.18261451306408605\n",
      "7 0.10891226682563523\n",
      "7 0.06389061799237669\n",
      "7 0.003535624624381164\n",
      "7 0.0016994719439742667\n",
      "7 0.016731755660452093\n",
      "7 0.08120661285797418\n",
      "7 0.000937362069779013\n",
      "7 0.06300807088078696\n",
      "8 0.01569048720093484\n",
      "8 0.05136266738366593\n",
      "8 0.022214041204926503\n",
      "8 0.0387240654394002\n",
      "8 0.0024426529404489726\n",
      "8 0.0006670519150666721\n",
      "8 0.012609428508570808\n",
      "8 0.007391133673933474\n",
      "8 0.02999953034635805\n",
      "8 0.0557712464879906\n",
      "8 0.21592332631968347\n",
      "8 0.03751797743108902\n",
      "8 0.09531620183901975\n",
      "8 0.14529415329265333\n",
      "8 0.002092098309358228\n",
      "8 0.0011683441125174099\n",
      "8 0.004228908113969893\n",
      "8 0.10907850304937973\n",
      "8 0.0009906427589822541\n",
      "8 0.038990870291567566\n",
      "9 0.024157189723326554\n",
      "9 0.010658984986490126\n",
      "9 0.006529545846430515\n",
      "9 0.008101087967878513\n",
      "9 0.02221204156038464\n",
      "9 0.0006749220364483932\n",
      "9 0.01566512689981636\n",
      "9 0.009590505517792423\n",
      "9 0.010867161735804522\n",
      "9 0.03498210054884758\n",
      "9 0.19250294677842855\n",
      "9 0.018998414154177076\n",
      "9 0.03259284525621862\n",
      "9 0.026415922002769748\n",
      "9 0.0009354692295745437\n",
      "9 0.0007715334125787975\n",
      "9 0.0009344503590685349\n",
      "9 0.11282726821438274\n",
      "9 0.0007152346738507376\n",
      "9 0.04814836050439389\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "mini_batch_size = 50\n",
    "epoch = 10\n",
    "\n",
    "criterion = LossMSE()\n",
    "\n",
    "for e in range(epoch):\n",
    "    for b in range(0, X_train.size(0), mini_batch_size):\n",
    "        output = model.forward(X_train.narrow(0, b, mini_batch_size))\n",
    "        #print(output)\n",
    "        #predict = output.max(1)[1]\n",
    "        loss = criterion.calculate(output, y_train.narrow(0, b, mini_batch_size).float())\n",
    "        print(e, loss)\n",
    "        l_grad = criterion.backward()\n",
    "        #print(l_grad)\n",
    "        model.backward(l_grad)\n",
    "        for layer in model.param():\n",
    "            for p in layer:\n",
    "                p[0].sub_(lr*p[1])\n",
    "    #print(e, criterion.calculate(model.forward(X_train), y_train.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 1.90%\n",
      "Test Error: 1.20%\n"
     ]
    }
   ],
   "source": [
    "train_output = model.forward(X_train)\n",
    "print(\"Train Error: {:.2%}\".format(train_output.max(1)[1].ne(y_train.max(1)[1]).sum()/train_output.size(0)))\n",
    "\n",
    "test_output = model.forward(X_test)\n",
    "print(\"Test Error: {:.2%}\".format(test_output.max(1)[1].ne(y_test.max(1)[1]).sum()/test_output.size(0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
